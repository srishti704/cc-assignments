{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "name": ""
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "939f14db-c0ea-42cf-9af5-2cf7e2f8b0ef",
      "cell_type": "code",
      "source": "Q1. Write a unique paragraph (5-6 sentences) about your favorite topic (e.g., sports, technology, food, books, etc.).\n1. Convert text to lowercase and remove punctuation.\n2. Tokenize the text into words and sentences.\n3. Remove stopwords (using NLTK's stopwords list).\n4. Display word frequency distribution (excluding stopwords).\n\nimport nltk\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.probability import FreqDist\n\nnltk.download('punkt')\nnltk.download('stopwords')\n\n# Paragraph\noriginal_text = \"\"\"Technology has always fascinated me. From artificial intelligence to state-of-the-art robotics, \nit's incredible how fast things are evolving! I enjoy reading about new gadgets, apps, and breakthrough innovations. \nTech blogs and YouTube reviews are my go-to sources. The future seems limitless with all this progress.\"\"\"\n\n# 1. Lowercase and remove punctuation\ntext_lower = original_text.lower()\ntext_clean = text_lower.translate(str.maketrans('', '', string.punctuation))\n\n# 2. Tokenize\nwords = word_tokenize(text_clean)\nsentences = sent_tokenize(original_text)\n\n# 3. Remove stopwords\nstop_words = set(stopwords.words('english'))\nfiltered_words = [word for word in words if word not in stop_words]\n\n# 4. Word frequency distribution\nfreq_dist = FreqDist(filtered_words)\nprint(\"Filtered Words:\", filtered_words)\nprint(\"Word Frequency:\")\nfor word, freq in freq_dist.items():\n    print(f\"{word}: {freq}\")\n\n\nQ2: Stemming and Lemmatization\n1. Take the tokenized words from Question 1 (after stopword removal).\n2. Apply stemming using NLTK's PorterStemmer and LancasterStemmer.\n3. Apply lemmatization using NLTK's WordNetLemmatizer.\n4. Compare and display results of both techniques.\n\nfrom nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\nnltk.download('wordnet')\n\nps = PorterStemmer()\nls = LancasterStemmer()\nlemmatizer = WordNetLemmatizer()\n\nporter_stems = [ps.stem(word) for word in filtered_words]\nlancaster_stems = [ls.stem(word) for word in filtered_words]\nlemmas = [lemmatizer.lemmatize(word) for word in filtered_words]\n\nprint(\"Porter Stemmer:\", porter_stems)\nprint(\"Lancaster Stemmer:\", lancaster_stems)\nprint(\"Lemmatized:\", lemmas)\n\n\nQ3. Regular Expressions and Text Splitting\n1. Take the original text from Question 1.\n2. Use regular expressions to:\na. Extract all words with more than 5 letters.\nb. Extract all numbers (if any exist).\nc. Extract all capitalized words.\n3. Use text splitting techniques to:\na. Split the text into words containing only alphabets.\nb. Extract words starting with a vowel.\n\nimport re\n\n# a. Words with more than 5 letters\nlong_words = re.findall(r'\\b[a-zA-Z]{6,}\\b', original_text)\n\n# b. Extract numbers\nnumbers = re.findall(r'\\d+', original_text)\n\n# c. Extract capitalized words\ncapitalized_words = re.findall(r'\\b[A-Z][a-z]*\\b', original_text)\n\n# d. Words containing only alphabets\nalphabet_words = re.findall(r'\\b[a-zA-Z]+\\b', original_text)\n\n# e. Words starting with a vowel\nvowel_words = re.findall(r'\\b[aeiouAEIOU][a-zA-Z]*\\b', original_text)\n\nprint(\"Words > 5 letters:\", long_words)\nprint(\"Numbers:\", numbers)\nprint(\"Capitalized Words:\", capitalized_words)\nprint(\"Alphabet-only Words:\", alphabet_words)\nprint(\"Words Starting with Vowel:\", vowel_words)\n\n\nQ4. Custom Tokenization & Regex-based Text Cleaning\n1. Take original text from Question 1.\n2. Write a custom tokenization function that:\na. Removes punctuation and special symbols but keeps contractions.\nb. Keeps hyphenated words as a single token.\nc. Tokenizes numbers separately but keeps decimal numbers intact.\n3. Use Regex Substitutions (re.sub) to:\na. Replace email addresses with <EMAIL>.\nb. Replace URLs with <URL>.\nc. Replace phone numbers with <PHONE>.\n\ndef custom_tokenize(text):\n    # Replace emails\n    text = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w{2,4}\\b', '<EMAIL>', text)\n    # Replace URLs\n    text = re.sub(r'https?://\\S+|www\\.\\S+', '<URL>', text)\n    # Replace phone numbers\n    text = re.sub(r'(\\+?\\d{1,3}[ -]?)?\\d{3}[- ]?\\d{3}[- ]?\\d{4}', '<PHONE>', text)\n\n    # Remove unwanted punctuation (but keep hyphens and contractions)\n    text = re.sub(r\"[^\\w\\s\\-']\", ' ', text)\n\n    # Tokenize: keep decimals, hyphenated words, contractions\n    tokens = re.findall(r\"\\d+\\.\\d+|\\w+(?:-\\w+)*|'[a-z]+|\\w+\", text)\n    return tokens\n\ncustom_tokens = custom_tokenize(original_text)\nprint(\"Custom Tokens:\", custom_tokens)\n\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}