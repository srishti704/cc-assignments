{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "cc6ef81a-b8b0-45d1-b5a1-e06df542a56d",
      "cell_type": "code",
      "source": "Q1. Write a unique paragraph (5–6 sentences) about your favorite topic.\nLet's say your favorite topic is Technology\nParagraph:\nTechnology has transformed the world into a global village. With the rise of artificial intelligence and machine learning, machines are becoming more intelligent every day. Smartphones have become essential tools for both communication and productivity. Cloud computing allows people to access their data from anywhere in the world. Innovations like virtual reality and augmented reality are redefining entertainment and education.\n\nQ1 Solution:\nimport re\nimport nltk\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom collections import Counter\n\n# Ensure required NLTK data is downloaded\nnltk.download('punkt')\nnltk.download('stopwords')\n\nparagraph = \"\"\"Technology has transformed the world into a global village. With the rise of artificial intelligence and machine learning, machines are becoming more intelligent every day. Smartphones have become essential tools for both communication and productivity. Cloud computing allows people to access their data from anywhere in the world. Innovations like virtual reality and augmented reality are redefining entertainment and education.\"\"\"\n\n# 1. Lowercase and remove punctuation\nlowered = paragraph.lower()\ncleaned = re.sub(r'[^\\w\\s]', '', lowered)\n\n# 2. Tokenization\nword_tokens = word_tokenize(cleaned)\nsent_tokens = sent_tokenize(paragraph)\n\n# 3. Split vs word_tokenize\nsplit_tokens = cleaned.split()\nprint(\"Split tokens:\", split_tokens[:10])\nprint(\"Word_tokenize tokens:\", word_tokens[:10])\n\n# 4. Remove stopwords\nstop_words = set(stopwords.words('english'))\nfiltered_words = [w for w in word_tokens if w not in stop_words]\n\n# 5. Word frequency (excluding stopwords)\nfreq_dist = Counter(filtered_words)\nprint(\"Word Frequency Distribution (no stopwords):\", freq_dist)\n\n\nQ2. Using the same paragraph from Q1:\nQ2 Solution:\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n\n# 1. Extract alphabetic words only\nalphabetic_words = re.findall(r'\\b[a-zA-Z]+\\b', cleaned)\n\n# 2. Remove stop words\nfiltered_alpha = [w for w in alphabetic_words if w not in stop_words]\n\n# 3. Stemming\nstemmer = PorterStemmer()\nstemmed = [stemmer.stem(word) for word in filtered_alpha]\n\n# 4. Lemmatization\nlemmatizer = WordNetLemmatizer()\nlemmatized = [lemmatizer.lemmatize(word) for word in filtered_alpha]\n\n# 5. Comparison\nprint(\"Stemmed Words:\", stemmed[:10])\nprint(\"Lemmatized Words:\", lemmatized[:10])\n\n# Explanation:\nprint(\"\\nExplanation: Stemming is faster but less accurate (e.g., 'machines' becomes 'machin'), while Lemmatization gives meaningful roots (e.g., 'machines' → 'machine'). Prefer lemmatization for tasks needing correct grammar or human readability.\")\n\nQ3. Choose 3 short texts and analyze with BoW & TF-IDF\nQ3 Solution:\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\ntexts = [\n    \"The phone has excellent battery life and a sleek design.\",\n    \"The camera quality is poor, and the phone lags frequently.\",\n    \"Great value for money and smooth user interface.\"\n]\n\n# 1. Bag of Words\ncount_vect = CountVectorizer()\nbow_matrix = count_vect.fit_transform(texts)\nprint(\"Bag of Words:\\n\", bow_matrix.toarray())\n\n# 2. TF-IDF\ntfidf_vect = TfidfVectorizer()\ntfidf_matrix = tfidf_vect.fit_transform(texts)\nprint(\"TF-IDF Matrix:\\n\", tfidf_matrix.toarray())\n\n# 3. Top 3 keywords\nimport numpy as np\nfeature_names = tfidf_vect.get_feature_names_out()\nfor i, text in enumerate(texts):\n    print(f\"\\nTop keywords for Text {i+1}:\")\n    row = tfidf_matrix[i].toarray()[0]\n    top_indices = np.argsort(row)[-3:][::-1]\n    for idx in top_indices:\n        print(f\"{feature_names[idx]}: {row[idx]:.3f}\")\n\n\nQ4. Compare two technologies\nQ4 Solution:\ntext1 = \"Artificial Intelligence allows machines to mimic human intelligence and perform tasks autonomously.\"\ntext2 = \"Blockchain is a decentralized ledger technology that provides secure and transparent transactions.\"\n\n# Preprocessing\ndef preprocess(text):\n    text = re.sub(r'[^\\w\\s]', '', text.lower())\n    return word_tokenize(text)\n\ntokens1 = preprocess(text1)\ntokens2 = preprocess(text2)\n\n# a. Jaccard Similarity\nset1, set2 = set(tokens1), set(tokens2)\njaccard_sim = len(set1 & set2) / len(set1 | set2)\nprint(\"Jaccard Similarity:\", jaccard_sim)\n\n# b. Cosine Similarity\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ntfidf_vec = TfidfVectorizer()\ntfidf_matrix = tfidf_vec.fit_transform([text1, text2])\ncos_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\nprint(\"Cosine Similarity:\", cos_sim)\n\n# c. Analysis\nprint(\"Analysis: Cosine Similarity is better for long texts with different vocabularies but similar context, while Jaccard is good for short texts or simple word comparisons.\")\n\nQ5. Write and analyze a product review\nQ5 Solution:\nfrom textblob import TextBlob\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\nreview = \"I loved the customer support and the user-friendly interface. The product was amazing!\"\n\n# 1. Sentiment Analysis\nblob = TextBlob(review)\npolarity = blob.sentiment.polarity\nsubjectivity = blob.sentiment.subjectivity\nprint(\"Polarity:\", polarity, \"Subjectivity:\", subjectivity)\n\n# 2. Classification\nif polarity > 0.1:\n    sentiment = \"Positive\"\nelif polarity < -0.1:\n    sentiment = \"Negative\"\nelse:\n    sentiment = \"Neutral\"\nprint(\"Sentiment:\", sentiment)\n\n# 3. Word Cloud for positive reviews\nif sentiment == \"Positive\":\n    wordcloud = WordCloud(width=600, height=400, background_color='white').generate(review)\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.title('Word Cloud for Positive Review')\n    plt.show()\n\nQ6. Generate text using LSTM or Dense Model\nQ6 Solution:\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM, Dense\nimport numpy as np\n\n# Sample paragraph\ntrain_text = \"Deep learning is a subset of machine learning that uses neural networks to learn from data and make predictions.\"\n\n# 1. Tokenization\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts([train_text])\ntotal_words = len(tokenizer.word_index) + 1\ninput_sequences = []\n\n# 2. Create input sequences\ntokens = tokenizer.texts_to_sequences([train_text])[0]\nfor i in range(1, len(tokens)):\n    seq = tokens[:i+1]\n    input_sequences.append(seq)\n\n# Pad sequences\nmax_len = max([len(x) for x in input_sequences])\ninput_sequences = pad_sequences(input_sequences, maxlen=max_len, padding='pre')\n\nX = input_sequences[:, :-1]\ny = input_sequences[:, -1]\ny = np.array(y)\n\n# 3. Simple model\nmodel = Sequential()\nmodel.add(Embedding(total_words, 10, input_length=max_len-1))\nmodel.add(LSTM(50))\nmodel.add(Dense(total_words, activation='softmax'))\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\nmodel.fit(X, y, epochs=500, verbose=0)\n\n# Text generation\nseed = \"deep\"\nnext_words = 3\nfor _ in range(next_words):\n    token_list = tokenizer.texts_to_sequences([seed])[0]\n    token_list = pad_sequences([token_list], maxlen=max_len-1, padding='pre')\n    predicted = np.argmax(model.predict(token_list, verbose=0), axis=-1)\n    output_word = \"\"\n    for word, index in tokenizer.word_index.items():\n        if index == predicted:\n            output_word = word\n            break\n    seed += \" \" + output_word\n\nprint(\"Generated Text:\", seed)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}